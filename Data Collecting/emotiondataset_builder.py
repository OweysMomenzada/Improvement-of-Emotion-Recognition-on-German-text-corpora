import pandas as pd
import numpy as np

from itertools import chain
import urllib.request as urllib2
import codecs
import warnings
from deep_translator import GoogleTranslator

from nltk import tokenize
import nltk


nltk.download('punkt')

def api_call(word, label, corpus, limit, sentence_len):
    # call api to get sentences
    word_url = url_encode(word)
    url = "https://www.dwds.de/r/?q=" + word_url + "&limit=" + str(limit) + "&corpus="+ corpus + "&view=tsv"
    df = pd.read_csv(url, sep='\t', encoding="utf-8")

    # if word does not exist in corpus
    if df.Hit.isnull()[0] == True:
        raise Exception("word does not exist in corpus")
        
    # only get sentences with n words
    df = df[df['Hit'].str.split().str.len().lt(sentence_len)]
    # delete sentences less than 2 words
    df = df[~df['Hit'].str.split().str.len().lt(2)]

    # split sentences into unnested lists
    df = df.Hit.map(tokenize.sent_tokenize)
    df = list(chain.from_iterable(df))

    # just get the sentence the word appears in
    df = [s for s in df if any(xs in s for xs in [word])]

    # make df with corresponding emotion
    df = pd.DataFrame({'label': len(df) * [label],
                       'text_de': df})
    
    df['corpus'] = corpus
    df['keyword'] = word
    
    return df 


def del_negation(df):
    """Deletes negation

    Args:
        df: Dataframe

    Returns:
        df: Returns cleaned dataframe
    """
    df = df.dropna()
    df.reset_index(drop=True, inplace=True)
    df['text_de'] = df['text_de'].str.lower()
    
    df = df[~df['text_de'].str.contains('nicht')]
    df = df[~df['text_de'].str.contains('ohne')]
    df = df[~df['text_de'].str.contains('keine')]
    df = df[~df['text_de'].str.contains('kein')]
    df = df[~df['text_de'].str.contains('keinen')]
    df = df[~df['text_de'].str.contains('keinem')]
    df = df[~df['text_de'].str.contains('keiner')]
    df = df[~df['text_de'].str.contains('\?')]

    return df


def generate_example_text_based_on_keywords(word: str, label: str, limit=200, sentence_len=12):
    """
    A function used to generate sentences based on one (or more) words

    ...

    Attributes
    ----------
    word : str
        give a word(s) to get  the sentence the word(s) appears in

    corpus : str
        the used corpus of the API. (default=korpus21)

    label : str
        give a label to the word, which will be provided through output

    limit : int
        number of sentences being generated by the API (default=80)

    sentence_len : str
        length of sentence of a generated sentence by the API (default=12)
    """
    warnings.warn("Network connection must be consistently running. No Error handling")
    
    corpus_list = ['korpus21', 'blogs', 'zeit', 'untertitel']
    
    df = [api_call(word, label, corpus, limit, sentence_len) for corpus in corpus_list]
    df = pd.concat([i for i in df], ignore_index=True, sort=False)
        
    if df.empty:
        df = df.append([np.nan], ignore_index=True)
        return df

    df = del_negation(df)
    
    df.drop_duplicates(subset=['text_de'], inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    return df

def url_encode(word):
    """encodes a words of a url.
    """
    word = codecs.encode(word, 'utf-8')
    url_Word = urllib2.quote(word)

    return url_Word

def translate_texts(sentence):
    """
        A function used to translate sentences.
        Note, here a free version of the GoogleTranslator is used instead of DeepL.
    """
    warnings.warn("Network connection must be consistently running. No Error handling")

    return GoogleTranslator(source='en', target='de').translate(sentence)


if __name__ == "__main__":
    # example usage
    weak_supervision_data = generate_example_text_based_on_keywords("erfreuen", "joy", limit=200, sentence_len=12)
    translated_text = translate_texts("Hey, this is an example sentence")

    print("\n\n============= Example usage of the Weak Supervision approach ==============")
    print(weak_supervision_data)
    print("\n\n============= Example usage of the Neural Machine Translation approach ==============")
    print(translated_text)