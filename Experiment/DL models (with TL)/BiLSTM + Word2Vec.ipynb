{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NME2lbDVTW05"
   },
   "source": [
    "### Bidirectional Long Short-Term Memory model with Transfer Learning \n",
    "\n",
    "This notebook focuses on training and testing the Bidirectional Long Short-Term Memory model with Transfer Learning that were proposed in this paper. The model was implemented using TensorFlow and Genism to import Word2Vec embeddings.\n",
    "\n",
    "Please note that the import of  Word2Vec embeddings can take several minutes.\n",
    "\n",
    "Please keep in mind that these notebooks are primarily used for conducting experiments, live coding, and implementing and evaluating the approaches presented in the paper. As a result, the code in this notebook may not strictly adhere to best practice coding standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute once\n",
    "# ONLY IF USED ON LOCAL VIEW\n",
    "import os\n",
    "\n",
    "# Getting the parent directory\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55135,
     "status": "ok",
     "timestamp": 1687553938108,
     "user": {
      "displayName": "Ovice Moe",
      "userId": "10112058264607559430"
     },
     "user_tz": -120
    },
    "id": "3s_voFbxCLoT",
    "outputId": "d95d8c22-797d-4f79-ba1b-5a61fb51010f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/content/gdrive/MyDrive/Experiment/DL modelle (with TL)/cc.de.300.bin.gz',\n",
       " <http.client.HTTPMessage at 0x7f1c55d99d50>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import urllib.request\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# downloads the embeddings of word2vec. Note that the package is 4.2gb\n",
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz'\n",
    "file_path = '/content/gdrive/MyDrive/Experiment/DL models (with TL)/cc.de.300.bin.gz'\n",
    "# uncomment this to download locally\n",
    "#file_path = 'cc.de.300.bin.gz'\n",
    "\n",
    "# Download the file\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y8zwQr_D2NX"
   },
   "source": [
    "### Note that you need to unzip the file before executing the next cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DQE-dVDTW08"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def import_test_train(local):\n",
    "  \"\"\"\n",
    "  This imports the given word2vecs embeddings, train and testset locally or not and returns it.\n",
    "\n",
    "  :param local: If set to true, it will return the trainset from a local view. Otherwise it will open drive mount and attempts to connect to your\n",
    "  drive folders.\n",
    "  \"\"\"\n",
    "\n",
    "  assert type(local) == bool, f\"Type is not valid. Expected boolean, recieved: {type(local)}\"\n",
    "\n",
    "  if local:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    df_test = pd.read_csv('/content/gdrive/MyDrive/Experiment/testset_DE_Trigger.csv')\n",
    "    df_train = pd.read_csv('/content/gdrive/MyDrive/Experiment/trainset_DE_Trigger.csv')\n",
    "    vecs = KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Experiment/DL models (with TL)/cc.de.300.vec')\n",
    "\n",
    "    return df_test, df_train, vecs\n",
    "\n",
    "  else:\n",
    "    df_test = pd.read_csv('./Experiment/testset_DE_Trigger.csv')\n",
    "    df_train = pd.read_csv('./Experiment/trainset_DE_Trigger.csv')\n",
    "    vecs = KeyedVectors.load_word2vec_format('./Experiment/DL models (with TL)/cc.de.300.vec')\n",
    "\n",
    "    return df_test, df_train, vecs\n",
    "\n",
    "# importing test and trainset\n",
    "df_test, df_train, vecs = import_test_train(True)\n",
    "\n",
    "# If you want to use it locally, make sure to execute the notebooks from the root directory of this project and uncomment the following line:\n",
    "# df_test, df_train, vecs = import_test_train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raf9SQkfTW1A"
   },
   "source": [
    "## Define Labels as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1677421968438,
     "user": {
      "displayName": "Ovice Moe",
      "userId": "10112058264607559430"
     },
     "user_tz": -60
    },
    "id": "7hf4nuSfTW1A",
    "outputId": "c9a854f7-591e-4d71-eeb5-7319530854d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44770 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 39000\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df_test.append(df_train)[\"content\"].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyAiy3KgPb-V"
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(df_train['content'].values)\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "Y_train = pd.get_dummies(df_train['label']).values\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(df_test['content'].values)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "Y_test = pd.get_dummies(df_test['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1677421970056,
     "user": {
      "displayName": "Ovice Moe",
      "userId": "10112058264607559430"
     },
     "user_tz": -60
    },
    "id": "VRhn1rfmTg-e",
    "outputId": "5765f52f-a15c-469c-f651-cb2182e93317"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-cc5a43eb49c5>:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if word in vecs.wv.vocab:\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300\n",
    "gensim_weight_matrix = np.zeros((MAX_NB_WORDS ,vector_size))\n",
    "\n",
    "gensim_weight_matrix.shape\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < MAX_NB_WORDS: # since index starts with zero\n",
    "        if word in vecs.wv.vocab:\n",
    "            gensim_weight_matrix[index] = vecs[word]\n",
    "        else:\n",
    "            gensim_weight_matrix[index] = np.zeros(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bynSQXtrTW1E"
   },
   "source": [
    "## Define model and saving path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9UhJ-uLTW1F"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, BatchNormalization, Dropout, Bidirectional\n",
    "\n",
    "def emotion_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, HIDDEN_DIM, input_length=X_train.shape[1]))\n",
    "    model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer='adam',metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xGLFNZJTW1F"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sTVyvR6TW1G",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set the early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Create the model\n",
    "model = emotion_model()\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Fit the model with early stopping\n",
    "model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuhvA23_TW1G"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hQLqpxJTW1H"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_labels = [list(i).index(1) for i in Y_test]\n",
    "Y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "\n",
    "print(classification_report(y_labels, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
