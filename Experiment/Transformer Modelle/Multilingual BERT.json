{"cells":[{"cell_type":"markdown","source":["### Multilingual BERT model (see Section 3.3. of thesis)\n","\n","This notebook focuses on training and testing of the BERT model that were proposed in this master's thesis. The model was implemented using TensorFlows and HuggingFace.\n","\n","Please keep in mind that these notebooks are primarily used for conducting experiments, live coding, and implementing and evaluating the approaches presented in the thesis. As a result, the code in this notebook may not strictly adhere to best practice coding standards."],"metadata":{"id":"nySzz1VhYzeU"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"foN57SLmop78","outputId":"90eb1ada-11c5-4b7f-cfa2-3bedbc60c92d","executionInfo":{"status":"ok","timestamp":1677371971864,"user_tz":-60,"elapsed":21644,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DQE-dVDTW08","scrolled":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from numpy import array, argmax\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import AutoTokenizer, TFAutoModel\n","\n","def import_test_train(local):\n","  \"\"\"\n","  This imports the given train and testset locally or not and returns it.\n","\n","  :param local: If set to true, it will return the trainset from a local view. Otherwise it will open drive mount and attempts to connect to your\n","  drive folders.\n","  \"\"\"\n","\n","  assert type(local) == bool, f\"Type is not valid. Expected boolean, recieved: {type(local)}\"\n","\n","  if local:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","\n","    df_test = pd.read_csv('/content/gdrive/MyDrive/Experiment/testset_DE_Trigger.csv')\n","    df_train = pd.read_csv('/content/gdrive/MyDrive/Experiment/trainset_DE_Trigger.csv')\n","\n","    return df_test, df_train\n","\n","  else:\n","    import os\n","\n","    # Getting the parent directory\n","    current_directory = os.getcwd()\n","    os.chdir('..')\n","\n","    df_test = pd.read_csv('./Experiment/testset_DE_Trigger.csv')\n","    df_train = pd.read_csv('./Experiment/trainset_DE_Trigger.csv')\n","\n","    return df_test, df_train\n","\n","# importing test and trainset\n","df_test, df_train = import_test_train(True)\n","\n","# If you want to use it locally, make sure to execute the notebooks from the root directory of this project and uncomment the following line:\n","# df_test, df_train = import_test_train(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2987,"status":"ok","timestamp":1677372154637,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"},"user_tz":-60},"id":"CMyMp5Dj82wu","outputId":"ef9e98af-7c43-438f-e442-7abdb8ff343c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","bert = TFAutoModel.from_pretrained(\"bert-base-multilingual-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6YY2Qoeqcbf"},"outputs":[],"source":["# max length of berttokenizer  is 512\n","max_length=100\n","\n","#creating mask for tokens\n","Xids_train=np.zeros((df_train.shape[0],max_length))\n","Xmask_train=np.zeros((df_train.shape[0],max_length))\n","y_train=np.zeros((df_train.shape[0],1))\n","\n","#creating mask for tokens\n","Xids_test=np.zeros((df_test.shape[0],max_length))\n","Xmask_test=np.zeros((df_test.shape[0],max_length))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hf4nuSfTW1A"},"outputs":[],"source":["for i,sequence in enumerate(df_train['content']):\n","    tokens=tokenizer.encode_plus(sequence,max_length=max_length,padding='max_length',add_special_tokens=True,\n","                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n","                           return_tensors='tf')\n","\n","    Xids_train[i,:] = tokens['input_ids']\n","    Xmask_train[i,:] = tokens['attention_mask']\n","    y_train[i,0] = df_train.loc[i,'label_id']\n","\n","y_train = to_categorical(y_train)\n","\n","for i,sequence in enumerate(df_test['content']):\n","    tokens=tokenizer.encode_plus(sequence,max_length=max_length,padding='max_length',add_special_tokens=True,\n","                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n","                           return_tensors='tf')\n","\n","    Xids_test[i,:] = tokens['input_ids']\n","    Xmask_test[i,:] = tokens['attention_mask']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Di0JsOixTW1C"},"outputs":[],"source":["dataset=tf.data.Dataset.from_tensor_slices((Xids_train,Xmask_train,y_train))\n","\n","def map_func(input_ids,mask,labels):\n","    return {'input_ids':input_ids,'attention_mask':mask},labels\n","\n","dataset=dataset.map(map_func)\n","dataset=dataset.shuffle(100000).batch(64).prefetch(1000)\n","\n","DS_size=len(list(dataset))\n","\n","train=dataset.take(round(DS_size*0.90))\n","val=dataset.skip(round(DS_size*0.90))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLJ3rMrtTW1C"},"outputs":[],"source":["dataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))\n","\n","def map_func(input_ids,mask):\n","    return {'input_ids':input_ids,'attention_mask':mask}\n","\n","dataset_test=dataset_test.map(map_func)\n","# batching it to or the predictions will be multiplied by the shape\n","dataset_test=dataset_test.batch(64).prefetch(1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNtnoR0XTW1D"},"outputs":[],"source":["def emotion_model():\n","  input_ids=tf.keras.layers.Input(shape=(max_length,),name='input_ids',dtype='int32')\n","  input_mask=tf.keras.layers.Input(shape=(max_length,),name='attention_mask',dtype='int32')\n","\n","  embedding=bert(input_ids,attention_mask=input_mask)[0]\n","  x=tf.keras.layers.GlobalMaxPool1D()(embedding)\n","  x=tf.keras.layers.BatchNormalization()(x)\n","  x=tf.keras.layers.Dense(256,activation='relu')(x)\n","  x=tf.keras.layers.Dropout(0.2)(x)\n","  output=tf.keras.layers.Dense(5,activation='softmax')(x)\n","\n","  model=tf.keras.Model(inputs=[input_ids,input_mask],outputs=output)\n","\n","  model.layers[2].trainable=False\n","\n","  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n","              optimizer='adam',metrics=[tf.keras.metrics.AUC()])\n","\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"8qEIZ3EdTW1E"},"source":["## Define train and test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4955,"status":"ok","timestamp":1677372175650,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"},"user_tz":-60},"id":"Co9Rm5QvTW1E","outputId":"1c25e5e3-e590-485d-eaaf-84a9b1a4171c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 100)]        0           []                               \n","                                                                                                  \n"," attention_mask (InputLayer)    [(None, 100)]        0           []                               \n","                                                                                                  \n"," tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  177853440   ['input_ids[0][0]',              \n","                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 100,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n"," xPooling1D)                                                                                      \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n"," alization)                                                                                       \n","                                                                                                  \n"," dense (Dense)                  (None, 256)          196864      ['batch_normalization[0][0]']    \n","                                                                                                  \n"," dropout_74 (Dropout)           (None, 256)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            1285        ['dropout_74[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 178,054,661\n","Trainable params: 199,685\n","Non-trainable params: 177,854,976\n","__________________________________________________________________________________________________\n"]}],"source":["model = emotion_model()\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"bynSQXtrTW1E"},"source":["## Define model and saving path"]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","es = EarlyStopping(\n","    monitor='val_auc',\n","    patience=2,\n","    min_delta=0.0010,\n","    mode='max'\n",")"],"metadata":{"id":"kstQ6CcjfptV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5LJ2mckGsn-"},"outputs":[],"source":["model.fit(train,\n","          validation_data=val,\n","          epochs=30,\n","          callbacks=[es])"]},{"cell_type":"markdown","metadata":{"id":"OvLCn-grvsIV"},"source":["# Include Tails"]},{"cell_type":"code","source":["y_pred=model.predict(dataset_test)\n","y_pred_new = np.argmax(y_pred,axis=1)\n","y_true = df_test['label_id'].values"],"metadata":{"id":"lpse2Or3ZTtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import metrics\n","print(metrics.classification_report(y_true, y_pred_new))"],"metadata":{"id":"IrwCD53XZU-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_F2WKabjaRs"},"source":["# Create evaluation for each subset based on the fullset training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMBoS1gYjPrp"},"outputs":[],"source":["def create_dataset(df):\n","  Xids_test=np.zeros((df.shape[0],max_length))\n","  Xmask_test=np.zeros((df.shape[0],max_length))\n","\n","  for i,sequence in enumerate(df['content']):\n","    tokens=tokenizer.encode_plus(sequence,max_length=max_length,padding='max_length',add_special_tokens=True,\n","                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n","                           return_tensors='tf')\n","\n","    Xids_test[i,:] = tokens['input_ids']\n","    Xmask_test[i,:] = tokens['attention_mask']\n","\n","  dataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))\n","\n","  def map_func(input_ids,mask):\n","      return {'input_ids':input_ids,'attention_mask':mask}\n","\n","  dataset_test=dataset_test.map(map_func)\n","  dataset_test=dataset_test.batch(64).prefetch(1000)\n","\n","  return dataset_test"]},{"cell_type":"code","source":["df_tails = df_test[df_test.source == \"Tails\"]\n","y_true = df_tails['label_id'].values\n","df_tails = create_dataset(df_tails)\n","\n","y_pred=model.predict(df_tails)\n","y_pred_new = np.argmax(y_pred,axis=1)\n","\n","print(metrics.classification_report(y_true, y_pred_new))"],"metadata":{"id":"CEcgh4EoZWBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_dailydialog = df_test[df_test.source == \"DailyDialog\"]\n","y_true = df_dailydialog['label_id'].values\n","df_dailydialog = create_dataset(df_dailydialog)\n","\n","y_pred=model.predict(df_dailydialog)\n","y_pred_new = np.argmax(y_pred,axis=1)\n","\n","print(metrics.classification_report(y_true, y_pred_new))"],"metadata":{"id":"MYL6_5w3ZXPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_GoEmotions = df_test[df_test.source == \"GoEmotions\"]\n","y_true = df_GoEmotions['label_id'].values\n","df_GoEmotions = create_dataset(df_GoEmotions)\n","\n","y_pred=model.predict(df_GoEmotions)\n","y_pred_new = np.argmax(y_pred,axis=1)\n","\n","print(metrics.classification_report(y_true, y_pred_new))"],"metadata":{"id":"dxc_PDQXZYzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_Isear = df_test[df_test.source == \"Isear\"]\n","y_true = df_Isear['label_id'].values\n","df_Isear = create_dataset(df_Isear)\n","\n","y_pred=model.predict(df_Isear)\n","y_pred_new = np.argmax(y_pred,axis=1)\n","\n","print(metrics.classification_report(y_true, y_pred_new))"],"metadata":{"id":"Txshvd8yZZrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_emosti = df_test[df_test.source == \"Emotion-stimulus\"]\n","y_true = df_emosti['label_id'].values\n","df_emosti = create_dataset(df_emosti)\n","\n","y_pred=model.predict(df_emosti)\n","y_pred_new = np.argmax(y_pred,axis=1)\n","\n","print(metrics.classification_report(y_true, y_pred_new))"],"metadata":{"id":"WLaRyabwZala"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hATdlIzOFJaz"},"source":["# Test predictions"]},{"cell_type":"markdown","metadata":{"id":"mpbu8CxKFLVE"},"source":["_______________________\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVyzb7DUanHO"},"outputs":[],"source":["def predict_label(sentence):\n","\n","  seq = tokenizer.encode_plus(sentence,max_length=max_length,padding='max_length',add_special_tokens=True,\n","                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n","                           return_tensors='tf')\n","\n","  seq = [seq['input_ids'], seq['attention_mask']]\n","\n","  result = model.predict(seq)[0]\n","  result = [round(i,4) for i in result]\n","\n","  dict_res = {'anger':result[0],\n","              'fear':result[1],\n","              'joy':result[2],\n","              'neutral':result[3],\n","              'sadness':result[4]\n","  }\n","\n","  return dict_res\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4021,"status":"ok","timestamp":1677371602702,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"},"user_tz":-60},"id":"20D8TToKbC4N","outputId":"4c31930d-a2d7-4080-d886-3dbb5d48c2be"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 4s 4s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'anger': 0.0024,\n"," 'fear': 0.0005,\n"," 'joy': 0.9183,\n"," 'neutral': 0.0019,\n"," 'sadness': 0.0768}"]},"metadata":{},"execution_count":28}],"source":["predict_label('Das hat mich gefreut')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677371602702,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"},"user_tz":-60},"id":"v7mX_d2n_pLx","outputId":"0047d696-4c0b-4263-d307-686149b4b3de"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 84ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'anger': 0.0971,\n"," 'fear': 0.0058,\n"," 'joy': 0.1583,\n"," 'neutral': 0.0027,\n"," 'sadness': 0.7361}"]},"metadata":{},"execution_count":29}],"source":["predict_label('Das hat mich nicht gefreut.')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":545,"status":"ok","timestamp":1677371603242,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"},"user_tz":-60},"id":"BSWzkbrndsJ1","outputId":"4a5151f2-b6d2-4b83-a6eb-4a200d20de36"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 81ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'anger': 0.0031,\n"," 'fear': 0.063,\n"," 'joy': 0.0053,\n"," 'neutral': 0.1133,\n"," 'sadness': 0.8153}"]},"metadata":{},"execution_count":30}],"source":["predict_label('Tragischer Tod in der Innenstadt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677371603242,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"},"user_tz":-60},"id":"fBo0mgD6ds0P","outputId":"7d1e3fdb-0e49-42d7-d542-d5ffec24fb1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 76ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'anger': 0.0562,\n"," 'fear': 0.2647,\n"," 'joy': 0.0037,\n"," 'neutral': 0.3792,\n"," 'sadness': 0.2961}"]},"metadata":{},"execution_count":31}],"source":["predict_label('DEUTSCHE BANGEN IN AFGHANISTAN UM IHR LEBEN')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677371603243,"user":{"displayName":"Ovice Moe","userId":"10112058264607559430"},"user_tz":-60},"id":"mRg9j0trdtB2","outputId":"1f84ec17-b714-40fd-dfb0-8c876885aa13"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 91ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'anger': 0.017,\n"," 'fear': 0.0202,\n"," 'joy': 0.0798,\n"," 'neutral': 0.7249,\n"," 'sadness': 0.1581}"]},"metadata":{},"execution_count":32}],"source":["predict_label('Wetterbericht von heute')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}