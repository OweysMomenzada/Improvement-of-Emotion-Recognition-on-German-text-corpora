{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Iasd_jGe6H"
   },
   "source": [
    "### German BERT model for Crowdflower\n",
    "\n",
    "This notebook focuses on training and testing the BERT model that were proposed in this paper. The model was implemented using TensorFlows and HuggingFace.\n",
    "\n",
    "Please keep in mind that these notebooks are primarily used for conducting experiments, live coding, and implementing and evaluating the approaches presented in the thesis. As a result, the code in this notebook may not strictly adhere to best practice coding standards.\n",
    "\n",
    "\n",
    "*Here, training with GPU is required. Thus, either use Google Colab or setup you GPU properly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY IF USED ON LOCAL VIEW\n",
    "# only execute once\n",
    "import os\n",
    "\n",
    "# Getting the parent directory\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9238,
     "status": "ok",
     "timestamp": 1665573263519,
     "user": {
      "displayName": "Ovice Moe",
      "userId": "10112058264607559430"
     },
     "user_tz": -120
    },
    "id": "foN57SLmop78",
    "outputId": "2d65728b-f643-46ea-cd36-c8e9ff0fb261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 6.7 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 62.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 49.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22168,
     "status": "ok",
     "timestamp": 1687554516526,
     "user": {
      "displayName": "Ovice Moe",
      "userId": "10112058264607559430"
     },
     "user_tz": -120
    },
    "id": "3DQE-dVDTW08",
    "outputId": "e0d804b0-7a0a-4d03-d4ef-dae35fc7c093",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import re\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def import_test_train(local):\n",
    "  \"\"\"\n",
    "  This imports the given fullset and triggerset locally or not and returns it.\n",
    "\n",
    "  :param local: If set to true, it will return the sets from a local view. Otherwise it will open drive mount and attempts to connect to your\n",
    "  drive folders.\n",
    "  \"\"\"\n",
    "\n",
    "  assert type(local) == bool, f\"Type is not valid. Expected boolean, recieved: {type(local)}\"\n",
    "\n",
    "  if local:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    df = pd.read_csv('/content/gdrive/MyDrive/Experiment/translated_fullset.csv')\n",
    "    df_trigger = pd.read_csv('/content/gdrive/MyDrive/Experiment/triggerset.csv')\n",
    "\n",
    "    return df, df_trigger\n",
    "\n",
    "  else:\n",
    "    df = pd.read_csv('./Experiment/translated_fullset.csv')\n",
    "    df_trigger = pd.read_csv('./Experiment/triggerset.csv')\n",
    "\n",
    "    return df, df_trigger\n",
    "\n",
    "# importing test and trainset\n",
    "df, df_trigger = import_test_train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc6JUgByTuzr"
   },
   "outputs": [],
   "source": [
    "## Dataset cleaning\n",
    "df = df[~df['label'].str.contains('surprise')]\n",
    "df = df[~df['label'].str.contains('disgust')]\n",
    "df = df[~df['source'].str.contains('GoodNews')]\n",
    "df = df[~df['source'].str.contains('DailyDialog')]\n",
    "df = df[~df['source'].str.contains('Emotion-stimulus')]\n",
    "df = df[~df['source'].str.contains('Isear')]\n",
    "df = df[~df['source'].str.contains('GoEmotions')]\n",
    "df_trigger = df_trigger[~df_trigger['label'].str.contains('disgust')]\n",
    "df.reset_index(drop='True', inplace=True)\n",
    "df['content'] = df['content_de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtkFbeuqrGcL"
   },
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "df[\"label_id\"] = lb_make.fit_transform(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDpH6uoap1B9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "# train test splitting\n",
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "# start index at 0\n",
    "df_train.reset_index(inplace=True)\n",
    "df_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08wZE7YwkE37"
   },
   "outputs": [],
   "source": [
    "df_trigger_train, df_trigger_test = train_test_split(df_trigger, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5GXR_uXiWPT"
   },
   "outputs": [],
   "source": [
    "### SPLIT TESTSIZE AGAIN\n",
    "_, df_trigger_test = train_test_split(df_trigger_test, test_size=0.38, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMmjaasPaYoQ"
   },
   "outputs": [],
   "source": [
    "# # drop possible duplicates caused by appending triggerset\n",
    "df_train = df_train.drop_duplicates(subset=['content'])\n",
    "df_test = df_test.drop_duplicates(subset=['content'])\n",
    "\n",
    "# # reset index\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2068,
     "status": "ok",
     "timestamp": 1665573461143,
     "user": {
      "displayName": "Ovice Moe",
      "userId": "10112058264607559430"
     },
     "user_tz": -120
    },
    "id": "CMyMp5Dj82wu",
    "outputId": "76e64fa4-8520-4302-9e75-b1b6d25ebf58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dbmdz/bert-base-german-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at dbmdz/bert-base-german-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "bert = TFAutoModel.from_pretrained(\"dbmdz/bert-base-german-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6YY2Qoeqcbf"
   },
   "outputs": [],
   "source": [
    "# max length of berttokenizer  is 512\n",
    "max_length=100\n",
    "\n",
    "#creating mask for tokens\n",
    "Xids_train=np.zeros((df_train.shape[0],max_length))\n",
    "Xmask_train=np.zeros((df_train.shape[0],max_length))\n",
    "y_train=np.zeros((df_train.shape[0],1))\n",
    "\n",
    "#creating mask for tokens\n",
    "Xids_test=np.zeros((df_test.shape[0],max_length))\n",
    "Xmask_test=np.zeros((df_test.shape[0],max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hf4nuSfTW1A"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "for i,sequence in enumerate(df_train['content']):\n",
    "    tokens=tokenizer.encode_plus(sequence,max_length=max_length,padding='max_length',add_special_tokens=True,\n",
    "                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n",
    "                           return_tensors='tf')\n",
    "\n",
    "    Xids_train[i,:] = tokens['input_ids']\n",
    "    Xmask_train[i,:] = tokens['attention_mask']\n",
    "    y_train[i,0] = df_train.loc[i,'label_id']\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "for i,sequence in enumerate(df_test['content']):\n",
    "    tokens=tokenizer.encode_plus(sequence,max_length=max_length,padding='max_length',add_special_tokens=True,\n",
    "                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n",
    "                           return_tensors='tf')\n",
    "\n",
    "    Xids_test[i,:] = tokens['input_ids']\n",
    "    Xmask_test[i,:] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Di0JsOixTW1C"
   },
   "outputs": [],
   "source": [
    "dataset=tf.data.Dataset.from_tensor_slices((Xids_train,Xmask_train,y_train))\n",
    "\n",
    "def map_func(input_ids,mask,labels):\n",
    "    return {'input_ids':input_ids,'attention_mask':mask},labels\n",
    "\n",
    "dataset=dataset.map(map_func)\n",
    "dataset=dataset.shuffle(100000).batch(64).prefetch(1000)\n",
    "\n",
    "DS_size=len(list(dataset))\n",
    "\n",
    "train=dataset.take(round(DS_size*0.90))\n",
    "val=dataset.skip(round(DS_size*0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLJ3rMrtTW1C"
   },
   "outputs": [],
   "source": [
    "dataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))\n",
    "\n",
    "def map_func(input_ids,mask):\n",
    "    return {'input_ids':input_ids,'attention_mask':mask}\n",
    "\n",
    "dataset_test=dataset_test.map(map_func)\n",
    "# batching it to or the predictions will be multiplied by the shape\n",
    "dataset_test=dataset_test.batch(64).prefetch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNtnoR0XTW1D"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def emotion_model():\n",
    "  input_ids=tf.keras.layers.Input(shape=(max_length,),name='input_ids',dtype='int32')\n",
    "  input_mask=tf.keras.layers.Input(shape=(max_length,),name='attention_mask',dtype='int32')\n",
    "\n",
    "  embedding=bert(input_ids,attention_mask=input_mask)[0]\n",
    "  x=tf.keras.layers.GlobalMaxPool1D()(embedding)\n",
    "  x=tf.keras.layers.BatchNormalization()(x)\n",
    "  x=tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "  x=tf.keras.layers.Dropout(0.2)(x)\n",
    "  output=tf.keras.layers.Dense(4,activation='softmax')(x)\n",
    "\n",
    "  model=tf.keras.Model(inputs=[input_ids,input_mask],outputs=output)\n",
    "\n",
    "  model.layers[2].trainable=False\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer='adam',metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qEIZ3EdTW1E"
   },
   "source": [
    "## Define train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2618,
     "status": "ok",
     "timestamp": 1665573469567,
     "user": {
      "displayName": "Ovice Moe",
      "userId": "10112058264607559430"
     },
     "user_tz": -120
    },
    "id": "Co9Rm5QvTW1E",
    "outputId": "dfa43076-7f2a-41de-fc8f-aa4ed4c3a58d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109927680   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          196864      ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)           (None, 256)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4)            1028        ['dropout_75[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,128,644\n",
      "Trainable params: 199,428\n",
      "Non-trainable params: 109,929,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = emotion_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bynSQXtrTW1E"
   },
   "source": [
    "## Define model and saving path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kstQ6CcjfptV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=2,\n",
    "    min_delta=0.0010,\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5LJ2mckGsn-"
   },
   "outputs": [],
   "source": [
    "model.fit(train,\n",
    "          validation_data=val,\n",
    "          epochs=30,\n",
    "          callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2qQy8voysG3"
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joTM6mziysG5"
   },
   "outputs": [],
   "source": [
    "y_pred_new = np.argmax(y_pred,axis=1)\n",
    "y_true = df_test['label_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4phpxaoFysG6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjU03BpUysG7"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_true, y_pred_new))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
